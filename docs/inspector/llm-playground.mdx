---
title: "LLM Playground"
description: "Test your MCP server's behavior against various LLMs"
icon: "Robot"
---

Use the LLM playground to test your server against different LLMs. This is helpful to simulate how LLMs interpret your server and test for hallucinations.

The playground is also great for simulating agent behaviors. You can customize the system prompt, swap models, and configure system prompts and temperature as if you would building an agent.

<Frame>
  <img
    className="block"
    src="/images/playground/llm-playground.png"
    alt="MCPJam Inspector Logo"
    width="300"
  />
</Frame>

## Set up LLM Playground

You need to set up at least one LLM to use the playground. Go to the settings tab in the inspector and follow instructions from there.

### OpenAI

Get an API key from [OpenAI Platform](https://platform.openai.com/api-keys).

`gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-4`, `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-chat-latest`, `gpt-5-pro`, `gpt-5-codex`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `gpt-3.5-turbo`,`o3-mini`, `o3`, `o4-mini`, `o1`

<Note>
  GPT-5 models require organization verification. If you encounter access
  errors, visit [OpenAI
  Settings](https://platform.openai.com/settings/organization/general) and
  verify your organization. Access may take up to 15 minutes after verification.
</Note>

### Claude (Anthropic)

Get an API key from [Anthropic Console](https://console.anthropic.com/).

`claude-opus-4-0`, `claude-sonnet-4-0`, `claude-3-7-sonnet-latest`, `claude-3-5-sonnet-latest`, `claude-3-5-haiku-latest`

### Gemini

Get an API key from [Google AI Studio](https://aistudio.google.com/)

`gemini-2.5-pro`, `gemini-2.5-flash`, `gemini-2.5-flash-lite`, `gemini-2.0-flash-exp`, `gemini-1.5-pro`, `gemini-1.5-pro-002`, `gemini-1.5-flash`, `gemini-1.5-flash-002`, `gemini-1.5-flash-8b`, `gemini-1.5-flash-8b-001`, `gemma-3-2b`, `gemma-3-9b`, `gemma-3-27b`, `gemma-2-2b`, `gemma-2-9b`, `gemma-2-27b`, `codegemma-2b`, `codegemma-7b`

### Deepseek

Get an API key from [Deepseek Platform](https://platform.deepseek.com/)

`deepseek-chat`, `deepseek-reasoner`

### Mistral AI

Get an API key from [Mistral AI Console](https://console.mistral.ai/api-keys/)

`mistral-large-latest`, `mistral-small-latest`, `codestral-latest`, `ministral-8b-latest`, `ministral-3b-latest`

### OpenRouter

Get an API key from [OpenRouter Console](https://openrouter.ai/settings/keys)

Select from any tool-capable model in the dropdown.

### Ollama

Make sure you have [Ollama installed](https://ollama.com/), and the MCPJam Ollama URL configuration is pointing to your Ollama instance.

Start an Ollama instance with `ollama serve <model>`. MCPJam will automatically detect any Ollama models running.

### LiteLLM Proxy

Use [LiteLLM Proxy](https://docs.litellm.ai/docs/proxy/quick_start) to connect to 100+ LLMs through a unified OpenAI-compatible interface.

1. **Start LiteLLM Proxy**: Follow the [LiteLLM Proxy Quick Start Guide](https://docs.litellm.ai/docs/proxy/quick_start) to set up your proxy server
2. **Configure in MCPJam**: Go to Settings → LiteLLM card → Click "Configure"
3. **Enter Connection Details**:
   - **Base URL**: Your LiteLLM proxy URL (default: `http://localhost:4000`)
   - **API Key**: Your proxy API key (use the same key you use in your API requests)
   - **Model Aliases**: Comma-separated list of model names configured in your proxy (e.g., `gpt-3.5-turbo, claude-3-opus, gemini-pro`)

<Info>
  Use the exact model names that work with your LiteLLM proxy's
  `/v1/chat/completions` endpoint. These are typically the model names without
  provider prefixes (e.g., `gpt-3.5-turbo` instead of `openai/gpt-3.5-turbo`).
</Info>

**Example configuration:**

```yaml
# In your LiteLLM config.yaml
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
```

Then in MCPJam, use `gpt-3.5-turbo` as the model alias.

## Choose an LLM model

Once you've configured your LLM API keys, go to the Playground tab. On the bottom near the text input, you should see a LLM model selector. Select the model from the ones you've configured

<Frame>
  <img
    className="block"
    src="/images/playground/model-selector.png"
    alt="Model selector"
    width="300"
  />
</Frame>

## System prompt and temperature

You can configure the system prompt and temperature, just like you would building an agent. The temperature is defaulted to the default value of the LLM providers (Claude = 0, OpenAI = 1.0).

<Info>
  Higher temperature settings tend to hallucinate more with MCP interactions
</Info>

<Frame>
  <img
    className="block"
    src="/images/playground/prompt-config.png"
    alt="Model selector"
    width="300"
  />
</Frame>

## Elicitation support

MCPJam has elicitation support in the LLM playground. Any elicitation requests will be shown as a popup modal.

## Playground V2 (Experimental)

An experimental version of the playground is available at **Playground V2** in the sidebar. This version uses the `@ai-sdk/react` library for improved streaming and tool execution.

**Key differences:**
- Built on Vercel AI SDK's React hooks (`useChat`)
- Automatic tool execution with `stepCountIs` limits
- Separate SSE-based elicitation system
- Simplified message streaming architecture

Both playgrounds support the same models and features. V2 is under active development and may have different behavior or limitations.
