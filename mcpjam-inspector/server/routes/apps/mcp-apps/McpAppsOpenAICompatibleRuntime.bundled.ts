// This file is auto-generated by scripts/bundle-mcp-apps-openai-compatible-runtime.js
// Do not edit directly - modify server/routes/apps/mcp-apps/McpAppsOpenAICompatibleRuntime.ts instead

export const MCP_APPS_OPENAI_COMPATIBLE_RUNTIME_SCRIPT =
  '"use strict";\n(() => {\n  // server/routes/apps/mcp-apps/McpAppsOpenAICompatibleRuntime.ts\n  (function bootstrap() {\n    if (window.openai) return;\n    const CONFIG_ID = "openai-compat-config";\n    const readConfig = () => {\n      try {\n        const el = document.getElementById(CONFIG_ID);\n        if (!el) {\n          console.warn("[OpenAI Compat] Missing config element #" + CONFIG_ID);\n          return null;\n        }\n        return JSON.parse(el.textContent || "{}");\n      } catch (err) {\n        console.error("[OpenAI Compat] Failed to parse config", err);\n        return null;\n      }\n    };\n    const config = readConfig();\n    if (!config) return;\n    const { toolId, toolInput, toolOutput, theme, viewMode, viewParams } = config;\n    let callId = 0;\n    const pendingCalls = /* @__PURE__ */ new Map();\n    const pendingCheckoutCalls = /* @__PURE__ */ new Map();\n    const CALL_TIMEOUT_MS = 3e4;\n    const CHECKOUT_TIMEOUT_MS = 6e4;\n    const sendRequest = (method, params) => {\n      const id = ++callId;\n      window.parent.postMessage({ jsonrpc: "2.0", id, method, params }, "*");\n      return { id };\n    };\n    const sendNotification = (method, params) => {\n      window.parent.postMessage(\n        { jsonrpc: "2.0", method, params: params ?? {} },\n        "*"\n      );\n    };\n    const postHeight = /* @__PURE__ */ (() => {\n      let lastHeight = 0;\n      return (height) => {\n        const rounded = Math.round(height);\n        if (rounded <= 0 || rounded === lastHeight) return;\n        lastHeight = rounded;\n        sendNotification("ui/notifications/size-changed", {\n          height: rounded\n        });\n      };\n    })();\n    const measureAndNotifyHeight = () => {\n      try {\n        let contentHeight = 0;\n        if (document.body) {\n          const children = document.body.children;\n          for (let i = 0; i < children.length; i++) {\n            const child = children[i];\n            if (child.tagName === "SCRIPT" || child.tagName === "STYLE") continue;\n            const rect = child.getBoundingClientRect();\n            const bottom = rect.top + rect.height + window.scrollY;\n            contentHeight = Math.max(contentHeight, bottom);\n          }\n          const bodyStyle = window.getComputedStyle(document.body);\n          contentHeight += parseFloat(bodyStyle.marginBottom) || 0;\n          contentHeight += parseFloat(bodyStyle.paddingBottom) || 0;\n        }\n        if (contentHeight <= 0) {\n          const docEl = document.documentElement;\n          contentHeight = Math.max(\n            docEl ? docEl.scrollHeight : 0,\n            document.body ? document.body.scrollHeight : 0\n          );\n        }\n        postHeight(Math.ceil(contentHeight));\n      } catch {\n      }\n    };\n    const setupAutoResize = () => {\n      let scheduled = false;\n      const scheduleMeasure = () => {\n        if (scheduled) return;\n        scheduled = true;\n        requestAnimationFrame(() => {\n          scheduled = false;\n          measureAndNotifyHeight();\n        });\n      };\n      scheduleMeasure();\n      if (typeof ResizeObserver !== "undefined") {\n        const ro = new ResizeObserver(scheduleMeasure);\n        ro.observe(document.documentElement);\n        if (document.body) ro.observe(document.body);\n      } else {\n        window.addEventListener("resize", scheduleMeasure);\n      }\n      window.addEventListener("load", () => {\n        requestAnimationFrame(measureAndNotifyHeight);\n      });\n    };\n    const openaiAPI = {\n      toolInput: toolInput ?? {},\n      toolOutput: toolOutput ?? null,\n      theme: theme ?? "dark",\n      displayMode: "inline",\n      viewMode: viewMode ?? "inline",\n      viewParams: viewParams ?? {},\n      widgetState: null,\n      /**\n       * Call an MCP tool by name. Returns a Promise resolved when the\n       * host sends back the JSON-RPC response with the matching id.\n       */\n      callTool(name, args = {}) {\n        const { id } = sendRequest("tools/call", {\n          name,\n          arguments: args,\n          _meta: {}\n        });\n        return new Promise((resolve, reject) => {\n          pendingCalls.set(id, { resolve, reject });\n          setTimeout(() => {\n            if (pendingCalls.has(id)) {\n              pendingCalls.delete(id);\n              reject(new Error("Tool call timeout"));\n            }\n          }, CALL_TIMEOUT_MS);\n        });\n      },\n      /**\n       * Send a follow-up message to the host chat.\n       * Uses sendRequest (not notification) because ui/message is a JSON-RPC\n       * request in the MCP Apps spec — the AppBridge only dispatches requests\n       * (messages with an id) to its onmessage handler.\n       */\n      sendFollowUpMessage(opts) {\n        const prompt = typeof opts === "string" ? opts : opts?.prompt ?? "";\n        sendRequest("ui/message", {\n          role: "user",\n          content: [{ type: "text", text: prompt }]\n        });\n      },\n      /**\n       * Alias for sendFollowUpMessage (ChatGPT compat).\n       */\n      sendFollowupTurn(message) {\n        this.sendFollowUpMessage(message);\n      },\n      /**\n       * Notify the host of the widget\'s intrinsic height.\n       */\n      notifyIntrinsicHeight(height) {\n        const n = Number(height);\n        if (Number.isFinite(n) && n > 0) postHeight(n);\n      },\n      /**\n       * Open an external URL in a new tab.\n       * ui/open-link is a request in the spec; param is `url` (not `href`).\n       */\n      openExternal(options) {\n        const href = typeof options === "string" ? options : options?.href;\n        if (!href) throw new Error("href is required for openExternal");\n        sendRequest("ui/open-link", { url: href });\n      },\n      /**\n       * Request a display mode change (inline, fullscreen, pip).\n       * ui/request-display-mode is a request in the spec.\n       */\n      requestDisplayMode(options = {}) {\n        const mode = options.mode || "inline";\n        this.displayMode = mode;\n        sendRequest("ui/request-display-mode", { mode });\n        return { mode };\n      },\n      /**\n       * Store arbitrary widget state for persistence.\n       * Maps to ui/update-model-context which is a request expecting\n       * { content?: ContentBlock[], structuredContent?: Record }.\n       */\n      setWidgetState(state) {\n        this.widgetState = state;\n        sendRequest("ui/update-model-context", {\n          structuredContent: typeof state === "object" && state !== null ? state : { value: state }\n        });\n      },\n      /**\n       * Request a modal to be opened (ChatGPT-specific, notification).\n       */\n      requestModal(options) {\n        const opts = options ?? {};\n        sendNotification("openai/requestModal", {\n          title: opts.title,\n          params: opts.params,\n          anchor: opts.anchor,\n          template: opts.template\n        });\n      },\n      /**\n       * Request the widget to be closed (ChatGPT-specific, notification).\n       */\n      requestClose() {\n        sendNotification("openai/requestClose", { toolId });\n      },\n      // ── File Upload / Download ────────────────────────────────────────\n      // These use custom (non-JSON-RPC) postMessage types, matching the\n      // ChatGPT widget-runtime.ts protocol. The sandbox proxy and\n      // SandboxedIframe whitelist these message types alongside CSP\n      // violation messages.\n      _fileCallId: 0,\n      _pendingFileCalls: /* @__PURE__ */ new Map(),\n      /**\n       * Upload a file (image) to the host. Returns a fileId for later retrieval.\n       */\n      uploadFile(file) {\n        const ALLOWED_TYPES = ["image/png", "image/jpeg", "image/webp"];\n        const MAX_SIZE = 20 * 1024 * 1024;\n        if (!(file instanceof File)) {\n          return Promise.reject(new Error("uploadFile requires a File object"));\n        }\n        if (!ALLOWED_TYPES.includes(file.type)) {\n          return Promise.reject(\n            new Error(\n              `Unsupported file type: ${file.type}. Allowed: ${ALLOWED_TYPES.join(", ")}`\n            )\n          );\n        }\n        if (file.size > MAX_SIZE) {\n          return Promise.reject(\n            new Error(\n              `File too large. Maximum size: ${MAX_SIZE / 1024 / 1024}MB`\n            )\n          );\n        }\n        const id = ++this._fileCallId;\n        return new Promise((resolve, reject) => {\n          this._pendingFileCalls.set(id, { resolve, reject });\n          const reader = new FileReader();\n          reader.onload = () => {\n            const dataUrl = reader.result;\n            const base64 = dataUrl.split(",")[1];\n            window.parent.postMessage(\n              {\n                type: "openai:uploadFile",\n                callId: id,\n                toolId,\n                data: base64,\n                mimeType: file.type,\n                fileName: file.name\n              },\n              "*"\n            );\n          };\n          reader.onerror = () => {\n            this._pendingFileCalls.delete(id);\n            reject(new Error("Failed to read file"));\n          };\n          reader.readAsDataURL(file);\n          setTimeout(() => {\n            if (this._pendingFileCalls.has(id)) {\n              this._pendingFileCalls.delete(id);\n              reject(new Error("Upload timeout"));\n            }\n          }, 6e4);\n        });\n      },\n      /**\n       * Get a download URL for a previously uploaded file.\n       */\n      getFileDownloadUrl(options) {\n        if (!options || !options.fileId) {\n          return Promise.reject(new Error("fileId is required"));\n        }\n        const id = ++this._fileCallId;\n        return new Promise((resolve, reject) => {\n          this._pendingFileCalls.set(id, { resolve, reject });\n          window.parent.postMessage(\n            {\n              type: "openai:getFileDownloadUrl",\n              callId: id,\n              toolId,\n              fileId: options.fileId\n            },\n            "*"\n          );\n          setTimeout(() => {\n            if (this._pendingFileCalls.has(id)) {\n              this._pendingFileCalls.delete(id);\n              reject(new Error("getFileDownloadUrl timeout"));\n            }\n          }, 3e4);\n        });\n      },\n      /**\n       * Request a checkout flow (ACP — Agentic Checkout Protocol).\n       * Uses notification + callId pattern (same as requestModal) to avoid\n       * conflicts with AppBridge\'s JSON-RPC request handling.\n       */\n      requestCheckout(session) {\n        const id = ++callId;\n        sendNotification("openai/requestCheckout", { ...session, callId: id });\n        return new Promise((resolve, reject) => {\n          pendingCheckoutCalls.set(id, { resolve, reject });\n          setTimeout(() => {\n            if (pendingCheckoutCalls.has(id)) {\n              pendingCheckoutCalls.delete(id);\n              reject(new Error("Checkout request timeout"));\n            }\n          }, CHECKOUT_TIMEOUT_MS);\n        });\n      }\n    };\n    window.addEventListener("message", (event) => {\n      if (event.source !== window.parent) return;\n      const data = event.data;\n      if (!data || data.jsonrpc !== "2.0") return;\n      if (data.id != null && (data.result !== void 0 || data.error !== void 0)) {\n        const pending = pendingCalls.get(data.id);\n        if (pending) {\n          pendingCalls.delete(data.id);\n          if (data.error) {\n            pending.reject(\n              new Error(\n                typeof data.error === "string" ? data.error : data.error?.message ?? "Unknown error"\n              )\n            );\n          } else {\n            pending.resolve(data.result);\n          }\n        }\n        return;\n      }\n      if (data.method) {\n        const params = data.params ?? {};\n        switch (data.method) {\n          // MCP Apps bridge notification names (SEP-1865)\n          case "ui/notifications/tool-input":\n            openaiAPI.toolInput = params.arguments ?? params;\n            break;\n          case "ui/notifications/tool-input-partial":\n            openaiAPI.toolInput = params.arguments ?? params;\n            break;\n          case "ui/notifications/tool-result":\n            openaiAPI.toolOutput = params;\n            break;\n          case "ui/notifications/tool-cancelled":\n            break;\n          case "ui/notifications/host-context-changed":\n            if (params.theme) openaiAPI.theme = params.theme;\n            if (params.displayMode) openaiAPI.displayMode = params.displayMode;\n            break;\n          case "openai/requestCheckout:response": {\n            const pending = pendingCheckoutCalls.get(params.callId);\n            if (pending) {\n              pendingCheckoutCalls.delete(params.callId);\n              if (params.error) {\n                pending.reject(\n                  new Error(\n                    typeof params.error === "string" ? params.error : "Checkout failed"\n                  )\n                );\n              } else {\n                pending.resolve(params.result);\n              }\n            }\n            break;\n          }\n        }\n      }\n    });\n    window.addEventListener("message", (event) => {\n      if (event.source !== window.parent) return;\n      const data = event.data;\n      if (!data) return;\n      if (data.type === "openai:uploadFile:response" || data.type === "openai:getFileDownloadUrl:response") {\n        const pending = openaiAPI._pendingFileCalls.get(data.callId);\n        if (pending) {\n          openaiAPI._pendingFileCalls.delete(data.callId);\n          if (data.error) {\n            pending.reject(new Error(data.error));\n          } else {\n            pending.resolve(data.result);\n          }\n        }\n      }\n    });\n    const PROTOCOL_VERSION = "2026-01-26";\n    const initId = ++callId;\n    window.parent.postMessage(\n      {\n        jsonrpc: "2.0",\n        id: initId,\n        method: "ui/initialize",\n        params: {\n          appInfo: { name: "openai-compat", version: "1.0.0" },\n          appCapabilities: {},\n          protocolVersion: PROTOCOL_VERSION\n        }\n      },\n      "*"\n    );\n    pendingCalls.set(initId, {\n      resolve: (result) => {\n        const res = result;\n        if (res?.hostContext) {\n          const ctx = res.hostContext;\n          if (ctx.theme && typeof ctx.theme === "string")\n            openaiAPI.theme = ctx.theme;\n          if (ctx.displayMode && typeof ctx.displayMode === "string")\n            openaiAPI.displayMode = ctx.displayMode;\n        }\n        sendNotification("ui/notifications/initialized", {});\n      },\n      reject: () => {\n        sendNotification("ui/notifications/initialized", {});\n      }\n    });\n    Object.defineProperty(window, "openai", {\n      value: openaiAPI,\n      writable: false,\n      configurable: false,\n      enumerable: true\n    });\n    setupAutoResize();\n  })();\n})();\n';
